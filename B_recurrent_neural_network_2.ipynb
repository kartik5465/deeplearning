{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartik5465/deeplearning/blob/master/B_recurrent_neural_network_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "write a program to implement simple form of a recurrent neural network b. LSTM for sentiment analysis on datsets like UMICH SI 650 for similar"
      ],
      "metadata": {
        "id": "vngc8tv2-vCL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM4JHxnRfGOf",
        "outputId": "eef39e99-de57-42bf-b12b-ab35879c6397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk               # NLP toolkit\n",
        "import re                 # Library for Regular expression operations\n",
        "nltk.download('punkt')    # Download the Punkt sentence tokenizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "from collections import Counter\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "max_lines = 10000000\n",
        "pos = '/content/drive/MyDrive/Datasets/positive.txt'\n",
        "neg = '/content/drive/MyDrive/Datasets/negative.txt'"
      ],
      "metadata": {
        "id": "Z2IsB5lPfU6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lexicon(pos, neg):\n",
        "    lexicon = []\n",
        "    for fi in [pos, neg]:\n",
        "        with open(fi, 'r') as f:\n",
        "            contents = f.readlines()\n",
        "            for l in contents[:max_lines]:\n",
        "                all_words = word_tokenize(l.lower())\n",
        "                lexicon += list(all_words)\n",
        "\n",
        "    lexicon = [lemmatizer.lemmatize(i) for i in lexicon]\n",
        "    w_counts = Counter(lexicon)\n",
        "\n",
        "    l2 =[]\n",
        "    for w in w_counts:\n",
        "        if 1000 > w_counts[w] > 50:\n",
        "            l2.append(w)\n",
        "    return l2"
      ],
      "metadata": {
        "id": "erDupUOafU9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_handling(sample,lexicon,classification):\n",
        "    featureset = []\n",
        "    with open(sample,'r') as f:\n",
        "        contents = f.readlines()\n",
        "        for l in contents[:max_lines]:\n",
        "            current_words = word_tokenize(l.lower())\n",
        "            current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
        "            features = np.zeros(len(lexicon))\n",
        "            for word in current_words:\n",
        "                if word.lower() in lexicon:\n",
        "                    index_value = lexicon.index(word.lower())\n",
        "                    features[index_value] += 1\n",
        "\n",
        "            features = list(features)\n",
        "            featureset.append([features,classification])\n",
        "\n",
        "    return featureset"
      ],
      "metadata": {
        "id": "g4SQfH3efVAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lexicon = create_lexicon(pos,neg)\n",
        "features = []\n",
        "features += sample_handling(pos, lexicon,[1,0])\n",
        "features += sample_handling(neg, lexicon,[0,1])\n",
        "random.shuffle(features)\n",
        "features = np.array(features)\n",
        "\n",
        "testing_size = int(0.1*len(features))\n",
        "\n",
        "X_train = list(features[:,0][:-testing_size])\n",
        "y_train = list(features[:,1][:-testing_size])\n",
        "X_test = list(features[:,0][-testing_size:])\n",
        "y_test = list(features[:,1][-testing_size:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbPq5wkSfVCf",
        "outputId": "0caa678a-b54e-4fd9-cfd3-f138be1d5f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-1db0c126a5c5>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  features = np.array(features)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 10\n",
        "batch_size = 128\n",
        "h1 = 500\n",
        "h2 = 500\n",
        "n_classes = 2"
      ],
      "metadata": {
        "id": "vniwIEkjmlS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf"
      ],
      "metadata": {
        "id": "pe3WjzNfnVzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_input = tf.placeholder(dtype='float')\n",
        "y_input = tf.placeholder(dtype='float')\n",
        "\n",
        "hidden_1 = {'weight':tf.Variable(tf.random_normal([len(X_train[0]), h1])),\n",
        "                  'bias':tf.Variable(tf.random_normal([h1]))}\n",
        "\n",
        "hidden_2 = {'weight':tf.Variable(tf.random_normal([h1, h2])),\n",
        "                  'bias':tf.Variable(tf.random_normal([h2]))}\n",
        "\n",
        "output_layer = {'weight':tf.Variable(tf.random_normal([h2, n_classes])),\n",
        "                'bias':tf.Variable(tf.random_normal([n_classes])),}"
      ],
      "metadata": {
        "id": "TCpm3Qhmmsfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = tf.add(tf.matmul(x_input, hidden_1['weight']), hidden_1['bias'])\n",
        "l1 = tf.nn.relu(l1)\n",
        "\n",
        "l2 = tf.add(tf.matmul(l1, hidden_2['weight']), hidden_2['bias'])\n",
        "l2 = tf.nn.relu(l2)\n",
        "\n",
        "output = tf.matmul(l2, output_layer['weight']) + output_layer['bias']"
      ],
      "metadata": {
        "id": "CvPhDt9UnhhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y_input))\n",
        "opt = tf.train.AdamOptimizer().minimize(loss)\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_loss = 0\n",
        "        i = 0\n",
        "        while i < len(X_train):\n",
        "            start = i\n",
        "            end = i + batch_size\n",
        "            batch_x = np.array(X_train[start:end])\n",
        "            batch_y = np.array(y_train[start:end])\n",
        "\n",
        "            _, batch_loss = sess.run([opt, loss], feed_dict={x_input: batch_x, y_input: batch_y})\n",
        "            epoch_loss += batch_loss\n",
        "            i += batch_size\n",
        "\n",
        "        print('Epoch {}: loss {}'.format(epoch, epoch_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoEo4W5mnima",
        "outputId": "2b995886-eb2c-426e-a559-2155e1577b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss 1052.4118041992188\n",
            "Epoch 1: loss 804.5350036621094\n",
            "Epoch 2: loss 598.6573791503906\n",
            "Epoch 3: loss 416.4913024902344\n",
            "Epoch 4: loss 296.70587158203125\n",
            "Epoch 5: loss 272.0382385253906\n",
            "Epoch 6: loss 236.01861572265625\n",
            "Epoch 7: loss 187.36559295654297\n",
            "Epoch 8: loss 133.1355209350586\n",
            "Epoch 9: loss 76.30462455749512\n"
          ]
        }
      ]
    }
  ]
}